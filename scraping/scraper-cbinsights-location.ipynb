{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410a415b-2a07-42f4-8c97-2afa412bd84a",
   "metadata": {},
   "source": [
    "# Locations from CB Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecccc8d-766a-411a-bc73-ca268ab63a32",
   "metadata": {},
   "source": [
    "#### Load relevant URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d4d651-1d92-4bfc-abe3-b01e132da430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7258\n"
     ]
    }
   ],
   "source": [
    "urls = list()\n",
    "with open('./cbinsights/cbinsights_relevant_links.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        urls.append(line.strip())\n",
    "            \n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f14dce-d841-4d97-912b-cdcc2ba52bf5",
   "metadata": {},
   "source": [
    "#### Scrape CB Insights for location information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efde6fd-f8b7-487d-9e04-e321d9f71e6d",
   "metadata": {},
   "source": [
    "Location information is found under Headquarters Location under the Overview & Products tab.\n",
    "\n",
    "We tried using Geopy's API to determine if a name is a city or region but it kept timing out:\\\n",
    "https://www.tutorialspoint.com/how-to-get-geolocation-in-python\n",
    "https://geopy.readthedocs.io/en/stable/index.html#nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c039a891-7585-47d7-968c-983f3e42e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderUnavailable, GeocoderTimedOut, GeocoderRateLimited\n",
    "from time import sleep\n",
    "\n",
    "geolocator = Nominatim(user_agent = 'city-or-region', timeout = 5)\n",
    "\n",
    "def city_or_region_deprecated(name, url):\n",
    "    max_tries = 5\n",
    "    tries = 0\n",
    "    \n",
    "    while tries < max_tries:\n",
    "        try:\n",
    "            loc = geolocator.geocode(name)\n",
    "            if loc:\n",
    "                loc_type = loc.raw.get('addresstype')\n",
    "\n",
    "                if loc_type == 'city' or loc_type == 'town':\n",
    "                    return 'city'\n",
    "                if loc_type == 'state' or loc_type == 'administrative':\n",
    "                    return 'region'\n",
    "        except (GeocoderUnavailable, GeocoderTimedOut, GeocoderRateLimited) as e:\n",
    "            timeout = 5\n",
    "            print(f'Unable to classify {name} as city or region for startup at {url}. Retrying in {timeout} seconds.')\n",
    "            sleep(5)\n",
    "            tries += 1\n",
    "            \n",
    "    print(f'Unable to classify {name} as city or region for startup at {url}. Giving up.')        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbbc5c-c9f3-4955-b7bb-71cf4b475776",
   "metadata": {},
   "source": [
    "Instead we will compare each name to a master city list found at:\\\n",
    "https://github.com/FinNLP/cities-list/blob/master/list.txt\n",
    "\n",
    "There will be a few misclassifications using this method, but we can always go back and see what types of misclassifications occur and fix them on a case by case basis, or we can choose to ignore the misclassifications since they will happen with a small number of startups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c4135f-9ef4-43ed-b43d-095998abe603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['naugatuck', 'le coudray', 'elaine', 'vandenberg air force base', 'wakeman']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_city_set():\n",
    "    response = requests.get('https://raw.githubusercontent.com/FinNLP/cities-list/refs/heads/master/list.txt')\n",
    "    if response:\n",
    "        cities = response.text.splitlines()\n",
    "        return {city.strip().lower() for city in cities}\n",
    "    \n",
    "print(list(get_city_set())[:5])\n",
    "\n",
    "city_set = get_city_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de8a39c-01e2-45d5-9fdf-c522ca9b5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_or_region(name):\n",
    "    if name.strip().lower() in city_set:\n",
    "        return 'city'\n",
    "    else:\n",
    "        return 'region'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab48326-da67-4295-ada8-6b8ae152a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_number(s):\n",
    "    return bool(re.search(r'\\d', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f97ae630-decd-4c93-9c6d-4f16b439322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of pages processed.\n",
      "10% of pages processed.\n",
      "15% of pages processed.\n",
      "20% of pages processed.\n",
      "25% of pages processed.\n",
      "30% of pages processed.\n",
      "35% of pages processed.\n",
      "40% of pages processed.\n",
      "45% of pages processed.\n",
      "50% of pages processed.\n",
      "55% of pages processed.\n",
      "60% of pages processed.\n",
      "65% of pages processed.\n",
      "70% of pages processed.\n",
      "75% of pages processed.\n",
      "80% of pages processed.\n",
      "85% of pages processed.\n",
      "90% of pages processed.\n",
      "95% of pages processed.\n",
      "All 7258 pages processed.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "matchers = {\n",
    "    'mosaic_change': re.compile(r'[+-]\\d+'),\n",
    "}\n",
    "\n",
    "# Fetches and parses page\n",
    "def scrape(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    \n",
    "        if response.status_code == 429:\n",
    "            retry_after = response.headers.get('Retry-After')\n",
    "            if retry_after:\n",
    "                print(f'Too many requests: {url}. Status code: {response.status_code}. Retrying after {retry_after} seconds.')\n",
    "                sleep(int(retry_after))\n",
    "            else:\n",
    "                print(f'Too many requests: {url}. Status code: {response.status_code}. Aborting.')\n",
    "                sys.exit(1)\n",
    "        elif response.status_code == 404:\n",
    "            return None\n",
    "        elif response.status_code != 200:\n",
    "            print(f'Failed to retrieve {url}. Status code: {response.status_code}')\n",
    "            return None\n",
    "        else:\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "def clean_soup(full_soup):\n",
    "    if not full_soup:\n",
    "        return None\n",
    "    title_div = full_soup.find('div', class_='flex flex-col')\n",
    "    info_div = full_soup.find('div', {'data-test': 'section-component'})\n",
    "\n",
    "    soup = BeautifulSoup('<div></div>', 'html.parser').div\n",
    "\n",
    "    if title_div:\n",
    "        soup.append(title_div)\n",
    "    if info_div:\n",
    "        soup.append(info_div)\n",
    "        \n",
    "    return soup\n",
    "\n",
    "def extract(soup):\n",
    "    data = {\n",
    "        'name': None,\n",
    "        'website': None,\n",
    "        'cb_description': None,\n",
    "        'year_founded': None,\n",
    "        'mosaic_change': None,\n",
    "        'city': None,\n",
    "        'region': None,\n",
    "        'country': None,\n",
    "        'postal': None,\n",
    "    }\n",
    "    \n",
    "    # Extract name\n",
    "    name_match = soup.find('h1', class_='cbi-default pr-2 text-2xl font-medium text-black')\n",
    "    if name_match:\n",
    "        data['name'] = name_match.text.strip()\n",
    "        \n",
    "    # Extract website URL\n",
    "    website_match = soup.find('a', class_='color--blue padding--top--s text-sm font-medium')\n",
    "    if website_match:\n",
    "        data['website'] = website_match['href'].strip()\n",
    "       \n",
    "    # Extract year founded\n",
    "    year_founded_match = soup.find('div', class_='Kpi_kpiItem__2CwXD')\n",
    "    if year_founded_match:\n",
    "        year = year_founded_match.find_next('span').text.strip()\n",
    "        if contains_number(year):\n",
    "            data['year_founded'] = year\n",
    "        \n",
    "    # Extract description\n",
    "    cb_description_match = soup.find('p', {'data-test': 'description'})\n",
    "    if cb_description_match:\n",
    "        data['cb_description'] = cb_description_match.text.strip()\n",
    "        \n",
    "    # Extract mosaic change\n",
    "    mosaic_change_match = soup.find('div', class_='Kpi_mosaic__Ax_II')\n",
    "    if mosaic_change_match:\n",
    "        p_text = mosaic_change_match.find_next('p').text.strip()\n",
    "        match = re.search(matchers['mosaic_change'], p_text)\n",
    "        if match:\n",
    "            data['mosaic_change'] = match.group().strip()\n",
    "            \n",
    "    # Extract location\n",
    "    location_match = soup.find('address', {'data-test': 'address'})\n",
    "    if location_match:\n",
    "        city_region_zip_match = location_match.find('p', {'data-test': 'city-state-zip'})\n",
    "        if city_region_zip_match:\n",
    "            city = None\n",
    "            region = None\n",
    "            postal = None\n",
    "            \n",
    "            city_region_zip = city_region_zip_match.text.strip()\n",
    "            if city_region_zip:\n",
    "                list_city_region_zip = city_region_zip.split(',')[:-1]\n",
    "                list_city_region = [x.strip() for x in list_city_region_zip if not contains_number(x)]\n",
    "                list_postal = [x.strip() for x in list_city_region_zip if contains_number(x)]\n",
    "                if len(list_postal) > 0:\n",
    "                    postal = list_postal[0]\n",
    "\n",
    "                # Use geopy to decide whether it's a city or a region (state/province)\n",
    "                if (len(list_city_region)) == 1:\n",
    "                    for city_region in list_city_region:\n",
    "                        result = city_or_region(city_region)\n",
    "                        if result == 'city':\n",
    "                            city = city_region\n",
    "                        elif result == 'region':\n",
    "                            region = city_region\n",
    "                elif (len(list_city_region)) == 2:\n",
    "                    city = list_city_region[0]\n",
    "                    region = list_city_region[1]\n",
    "\n",
    "                if city:\n",
    "                    data['city'] = city\n",
    "                if region:\n",
    "                    data['region'] = region\n",
    "                if postal:\n",
    "                    data['postal'] = postal\n",
    "        \n",
    "        country_match = location_match.find('p', {'data-test': 'country'})\n",
    "        if country_match:\n",
    "            country = country_match.text.strip()\n",
    "            if country:\n",
    "                data['country'] = country\n",
    "        \n",
    "    return data\n",
    "\n",
    "# Generator that yields overviews\n",
    "def process_websites(urls):\n",
    "    for url in urls:\n",
    "        dirty_soup = scrape(url)\n",
    "        soup = clean_soup(dirty_soup)\n",
    "        if soup:            \n",
    "            yield extract(soup)\n",
    "        else:\n",
    "            yield None\n",
    "            \n",
    "# Store overviews in CSV\n",
    "def write_to_csv(overview, filename='../datasets/overviews.csv'):\n",
    "    fieldnames = [*overview]\n",
    "\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        if file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow(overview)\n",
    "\n",
    "def main(urls):\n",
    "    overviews = process_websites(urls)\n",
    "    processed_overviews = 0\n",
    "    total_overviews = len(urls)\n",
    "    previous_percent_processed = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            overview = next(overviews)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        processed_overviews += 1\n",
    "        if overview:\n",
    "            write_to_csv(overview)\n",
    "        \n",
    "        percent_processed = (processed_overviews / total_overviews) * 100\n",
    "        if percent_processed - previous_percent_processed >= 5:\n",
    "            print(f'{(percent_processed):.0f}% of pages processed.')\n",
    "            previous_percent_processed = percent_processed\n",
    "    \n",
    "        timeout = randint(0, 2)\n",
    "        sleep(timeout)\n",
    "        \n",
    "    print(f'All {total_overviews} pages processed.')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test_urls = ['https://www.cbinsights.com/company/unitree',\n",
    "                 'https://www.cbinsights.com/company/slice',\n",
    "                 'https://www.cbinsights.com/company/roman-health-ventures',\n",
    "                 'https://www.cbinsights.com/company/maven-clinic',\n",
    "                 'https://www.cbinsights.com/company/doctorbox',\n",
    "                 'https://www.cbinsights.com/company/healthtap',\n",
    "                ]\n",
    "    \n",
    "    urls = list()\n",
    "    with open('./cbinsights/cbinsights_relevant_links.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            urls.append(line.strip())\n",
    "    \n",
    "    urls = [url.replace('/financials', '') for url in urls]\n",
    "    \n",
    "    # If program crashes, leftoff is the last index that was processed\n",
    "    leftoff = 0\n",
    "    main(urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
